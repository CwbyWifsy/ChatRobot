from __future__ import annotations
from scripts.utils import make_collection_name_from_path
import argparse
import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import Iterable, List
from tqdm import tqdm

from app.services.embedding import EmbeddingService
from app.services.hashing import NovelHasher
from app.logger import configure_logging
from app.services.text_splitter import ChapterTextSplitter, Chunk
from app.services.vector_store import MilvusVectorStore, VectorRecord

logger = logging.getLogger(__name__)


def iter_text_files(directory: Path) -> Iterable[Path]:
    for path in sorted(directory.glob("**/*.txt")):
        if path.is_file():
            # convert_to_utf8(path)
            yield path


async def load_file(path: Path) -> str:
    return await asyncio.to_thread(path.read_text, "utf-8")


async def process_file(
        path: Path,
        embedding_service: EmbeddingService,
        vector_store: MilvusVectorStore,
        splitter: ChapterTextSplitter,
        hasher: NovelHasher,
        collection_name: str,
        extra_collection_name: str | None,
        force: bool,
) -> None:
    logger.info("Reading %s", path)

    extra_store = None
    if extra_collection_name:
        logger.info("ä¸ºæ–‡ä»¶ %s ä½¿ç”¨ç‹¬ç«‹é›†åˆ %s", path.name, extra_collection_name)
        print(f"æœ¬ä¹¦ç‹¬ç«‹é›†åˆåï¼š{path.name} -> {extra_collection_name}")
        extra_store = MilvusVectorStore(collection_name=extra_collection_name)

    content = await load_file(path)

    book_title = path.stem
    logger.info("å½“å‰æ–‡ä»¶ä¹¦åè‡ªåŠ¨è®¾ç½®ä¸ºï¼š%s", book_title)

    file_hash = hasher.hash_file(path, extra_values=[book_title])
    if not force and vector_store.has_file(file_hash, collection_name):
        logger.info("æ£€æµ‹åˆ° %s å·²ä¸Šä¼ è¿‡ï¼ŒæœªæŒ‡å®š --forceï¼Œè‡ªåŠ¨è·³è¿‡", path)
        return

    # 1âƒ£ï¸ åˆ‡åˆ†æ–‡æœ¬ â€”â€” å¸¦è¿›åº¦æ¡
    logger.info("æ­£åœ¨åˆ‡åˆ†ç« èŠ‚â€¦")
    raw_chunks = list(splitter.split(content, book_title=book_title, source_path=path))
    chunks = []

    for c in tqdm(raw_chunks, desc="ğŸ“‘ åˆ†ç‰‡å¤„ç†ä¸­"):
        chunks.append(c)

    # 2âƒ£ï¸ åˆ†æ‰¹ embedding + ä¸Šä¼ ï¼Œä¸‰ä¸ªçœŸå®è¿›åº¦æ¡
    BATCH_SIZE = 1000  # æ¯æ‰¹å¤„ç†å¤šå°‘æ¡ï¼Œå¯ä»¥æŒ‰æœºå™¨æƒ…å†µæ”¹

    total = len(chunks)
    logger.info("æ­£åœ¨åˆ†æ‰¹ç”Ÿæˆå‘é‡å¹¶å†™å…¥ Milvus...")

    MAX_BOOK_TITLE_LEN = 256
    MAX_CHAPTER_TITLE_LEN = 512
    MAX_SOURCE_PATH_LEN = 256
    MAX_FILE_HASH_LEN = 128
    MAX_CONTENT_LEN = 8192

    # â‘  æ€»ä½“è¿›åº¦æ¡ï¼šæ•´æœ¬å°è¯´çš„åˆ†ç‰‡æ€»è¿›åº¦
    with tqdm(total=total, desc="ğŸ“¦ æ€»ä½“è¿›åº¦", unit="chunk") as pbar_total:
        for start in range(0, total, BATCH_SIZE):
            end = min(start + BATCH_SIZE, total)
            batch_chunks = chunks[start:end]
            batch_texts = [c.content for c in batch_chunks]

            # â‘¡ å½“å‰æ‰¹æ¬¡ embedding è¿›åº¦æ¡ï¼ˆåµŒå…¥ n æ¡ï¼‰
            batch_embeddings: List[List[float]] = []
            for text in tqdm(batch_texts, desc="ğŸ§  æœ¬æ‰¹ embedding", leave=False):
                vec = embedding_service.embed_documents([text])[0]
                batch_embeddings.append(vec)

            # ç»„è£…å½“å‰æ‰¹æ¬¡è®°å½•
            batch_records: list[VectorRecord] = []
            for chunk, embedding in zip(batch_chunks, batch_embeddings):
                chapter_title = chunk.chapter_title
                if len(chapter_title) > MAX_CHAPTER_TITLE_LEN:
                    logger.warning("è·³è¿‡ä¸€æ¡è®°å½•ï¼šchapter_title_len=%d, title=%r", len(chapter_title),
                                   chapter_title[:80])
                    continue
                batch_records.append(
                    VectorRecord(
                        content=chunk.content,
                        embedding=embedding,
                        book_title=chunk.book_title,
                        chapter_title=chunk.chapter_title,
                        chunk_index=chunk.chunk_index,
                        source_path=str(chunk.source_path),
                        file_hash=file_hash,
                    )
                )

            vector_store.insert_records(batch_records, collection_name)
            # å¦‚æœç”¨æˆ·å¯ç”¨äº† single_collectionï¼Œå†å†™å…¥æ–°é›†åˆ
            if extra_store is not None:
                extra_store.insert_records(batch_records)
            # æ›´æ–°æ€»ä½“è¿›åº¦
            pbar_total.update(len(batch_chunks))

    logger.info("å·²å‘é›†åˆ %s å†™å…¥ %d ä¸ªåˆ†ç‰‡", collection_name, total)
    if extra_collection_name:
        logger.info("å·²å‘ç‹¬ç«‹é›†åˆ %s é¢å¤–å†™å…¥ %d ä¸ªåˆ†ç‰‡", extra_collection_name, total)


async def main() -> None:
    parser = argparse.ArgumentParser(description="Upload UTF-8 novels into Milvus vector store")
    parser.add_argument("directory", type=Path, help="Directory containing .txt novel files")
    parser.add_argument("--collection", type=str, default=None, help="Target collection name")
    parser.add_argument("--force", action="store_true", help="Upload even if file hash already exists")
    parser.add_argument("--single_collection", action="store_true",
                        help="ä¸ºå½“å‰ä¸Šä¼ é¢å¤–åˆ›å»ºå¹¶å†™å…¥ä¸€ä¸ªæ–°é›†åˆ")
    args = parser.parse_args()

    directory: Path = args.directory
    if not directory.exists():
        raise SystemExit(f"ç›®å½• {directory} ä¸å­˜åœ¨")

    configure_logging()

    embedding_service = EmbeddingService()
    vector_store = MilvusVectorStore(collection_name=args.collection)
    target_collection = args.collection or vector_store.collection_name

    if not args.collection:
        collections = vector_store.list_collections()
        if collections:
            logger.info("å½“å‰å…±æœ‰ %d ä¸ªé›†åˆï¼š", len(collections))
            for name in collections:
                try:
                    novels = vector_store.list_books(name)
                except Exception as exc:  # pragma: no cover - è¿è¡Œç¯å¢ƒå¯èƒ½ç¼ºå°‘æƒé™
                    logger.warning("è¯»å–é›†åˆ %s çš„å°è¯´åˆ—è¡¨å¤±è´¥ï¼š%s", name, exc)
                    novels = []
                if novels:
                    logger.info("  - %s (%d æœ¬)ï¼š%s", name, len(novels), ", ".join(novels))
                else:
                    logger.info("  - %s (æš‚æ— å°è¯´)", name)
        else:
            logger.info("å½“å‰æ²¡æœ‰é›†åˆï¼Œå°†åˆ›å»ºé»˜è®¤é›†åˆ %s", target_collection)

        chosen = input(f"è¯·è¾“å…¥ç›®æ ‡é›†åˆåç§°ï¼ˆç›´æ¥å›è½¦ä½¿ç”¨ {target_collection}ï¼‰: ").strip()
        if chosen:
            target_collection = chosen
    else:
        logger.info(f"å…¬å…±é›†åˆ:{args.collection}")
    vector_store.use_collection(target_collection)
    logger.info("ä¸Šä¼ ç›®æ ‡é›†åˆï¼š%s", target_collection)

    splitter = ChapterTextSplitter()
    hasher = NovelHasher()

    # å…ˆæŠŠæ‰€æœ‰è¦å¤„ç†çš„ txt æ–‡ä»¶æ‹¿å‡ºæ¥
    all_files = list(iter_text_files(directory))
    if not all_files:
        logger.info("ç›®å½• %s ä¸‹æ²¡æœ‰æ‰¾åˆ° txt æ–‡ä»¶", directory)
        return

    # å¦‚æœéœ€è¦æ¯æœ¬å°è¯´å•ç‹¬å»º collectionï¼Œå…ˆåˆ—å‡ºå…¨éƒ¨åå­—è®©ä½ ç¡®è®¤
    per_file_extra: dict[Path, str] = {}
    if args.single_collection:
        print("ä½ å¯ç”¨äº† --single_collectionï¼Œå°†ä¸ºæ¯æœ¬å°è¯´åˆ›å»ºç‹¬ç«‹é›†åˆã€‚")
        print("å³å°†ä½¿ç”¨å¦‚ä¸‹æ˜ å°„ï¼š")

        for f in all_files:
            cname = make_collection_name_from_path(f)
            per_file_extra[f] = cname
            print(f"  {f.name}  ->  {cname}")

        confirm = input("ç¡®è®¤ä»¥ä¸Šé›†åˆåæ˜ å°„æ— è¯¯åç»§ç»­ï¼Ÿ[y/N]: ").strip().lower()
        if confirm not in {"y", "yes"}:
            raise SystemExit("å·²å–æ¶ˆä¸Šä¼ ã€‚")
    # sys.exit()
    for file_path in iter_text_files(directory):
        extra_name = per_file_extra.get(file_path) if args.single_collection else None
        await process_file(
            file_path,
            embedding_service=embedding_service,
            vector_store=vector_store,
            splitter=splitter,
            hasher=hasher,
            collection_name=vector_store.collection_name,
            extra_collection_name=extra_name,
            force=args.force,
        )


if __name__ == "__main__":
    asyncio.run(main())
